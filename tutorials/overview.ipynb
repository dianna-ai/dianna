{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img width=\"150\" alt=\"Logo_ER10\" src=\"https://user-images.githubusercontent.com/3244249/151994514-b584b984-a148-4ade-80ee-0f88b0aefa45.png\">\n",
    "\n",
    "# General overview of using dianna\n",
    "\n",
    "DIANNA is a Python package that brings explainable AI (XAI) to your research project. It wraps carefully selected XAI methods (explainers) in a simple, uniform interface. It's built by, with and for (academic) researchers and research software engineers working on machine learning projects.\n",
    "\n",
    "This overview illustrates the main strenghts of DIANNA, namely supporting many data modalities and several explainers. DIANNA is future-proof by supporting and advocating the ONNX de-facto standard for Neural Network models. Many modern frameworks alpready support native export to ONNX, for tutorials on conversion from PyTorch, Keras, Scikit-learn and TensorFlow see [conversion_onnx](https://github.com/dianna-ai/dianna/tree/main/tutorials/conversion_onnx) folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **General workflow**\n",
    "\n",
    "1. Provide your *trained model* and *data item* ( *text, image, time series or tabular* )\n",
    "\n",
    "```python\n",
    "model_path = 'your_model.onnx'  # model trained on your data modality\n",
    "data_item = <data_item> # data item for which the model's prediction needs to be explained \n",
    "```\n",
    "\n",
    "2. If the task is classification: which are the *classes* your model has been trained for?\n",
    "\n",
    "```python \n",
    "labels = [class_a, class_b]   # example of binary classification labels\n",
    "```\n",
    "*Which* of these classes do you want an explanation for?\n",
    "```python\n",
    "explained_class_index = labels.index(<explained_class>)  # explained_class can be any of the labels\n",
    "```\n",
    "\n",
    "3. Run dianna with the *explainer* of your choice ( *'LIME', 'RISE' or 'KernalSHAP'*) and visualize the output:\n",
    "\n",
    "```python\n",
    "explanation = dianna.<explanation_function>(model_path, data_item, explainer)\n",
    "dianna.visualization.<visualization_function>(explanation[explained_class_index], data_item)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_in_colab = 'google.colab' in str(get_ipython())\n",
    "if running_in_colab:\n",
    "  # install dianna\n",
    "  !python3 -m pip install dianna[notebooks]\n",
    "  \n",
    "  # download data used in this demo\n",
    "  import os \n",
    "  base_url = 'https://raw.githubusercontent.com/dianna-ai/dianna/main/dianna/'\n",
    "  paths_to_download = ['./data/binary-mnist.npz', './models/mnist_model_tf.onnx']\n",
    "  for path in paths_to_download:\n",
    "      !wget {base_url + path} -P {os.path.dirname(path)}\n",
    "    \n",
    "  paths_to_download = ['./data/movie_reviews_word_vectors.txt', './models/movie_review_model.onnx']\n",
    "  for path in paths_to_download:\n",
    "      !wget {base_url + path} -P {os.path.dirname(path)}    \n",
    "\n",
    "  base_url = 'https://raw.githubusercontent.com/dianna-ai/dianna/main/dianna/'\n",
    "  paths_to_download = ['./data/bee.jpg']\n",
    "  for path in paths_to_download:\n",
    "      !wget {base_url + path} -P {os.path.dirname(path)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # disable warnings relateds to versions of tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# for explanation and visualization\n",
    "import dianna\n",
    "from dianna import visualization\n",
    "from dianna import utils\n",
    "from dianna.utils.tokenizers import SpacyTokenizer\n",
    "\n",
    "# ONNX\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "# text-related\n",
    "import spacy\n",
    "from torchtext.vocab import Vectors\n",
    "from scipy.special import softmax\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "# keras model and preprocessing tools for Image\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from keras import backend as K\n",
    "from keras import utils\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data modalities**\n",
    "DIANNA supports text, images, time-series and tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Text example* \n",
    "\n",
    "Lets illustrate the general worksflow above with *textual* data. The data item of interest is a sentence being (a part of) a movie review and the [model](https://zenodo.org/record/5910598) has been trained to classify the movie reviews from the [Stanford sentiment treebank](https://nlp.stanford.edu/sentiment/index.html) <img width=\"20\" alt=\"nlp-logo_half_size\" src=\"https://user-images.githubusercontent.com/3244249/152355020-908c04f3-aa99-489d-b87a-7e6b1f586118.png\"> into 'positive' and 'negative' sentiment classes.\n",
    "We are interested in which words are contributing positively (red) and which - negatively (blue) towards the model's decision to classify the review as positive and we would like to use the [LIME](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf) explainer:\n",
    "\n",
    "*for a full example see the /Explainers/LIME/lime_text tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Provide your *trained model* and *text* of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model paths and labels\n",
    "model_path = Path('..','dianna','models', 'movie_review_model.onnx')\n",
    "word_vector_path = Path('..','dianna','labels', 'movie_reviews_word_vectors.txt')\n",
    "labels = (\"negative\", \"positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier accepts numerical tokens as input and outputs a score between 0 (the review is negative) and 1 (the review is positive).\n",
    "Therefore, we define a model runner class, which accepts a sentence as input instead and returns one of two classes: negative or positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the tokenizer for english is available\n",
    "spacy.cli.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieReviewsModelRunner:\n",
    "    def __init__(self, model, word_vectors, max_filter_size):\n",
    "        self.run_model = utils.get_function(str(model))\n",
    "        self.vocab = Vectors(word_vectors, cache=os.path.dirname(word_vectors))\n",
    "        self.max_filter_size = max_filter_size\n",
    "        \n",
    "        self.tokenizer =  SpacyTokenizer(name='en_core_web_sm')\n",
    "\n",
    "    def __call__(self, sentences):\n",
    "        # ensure the input has a batch axis\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = [sentences]\n",
    "\n",
    "        tokenized_sentences = []\n",
    "        for sentence in sentences:\n",
    "            # tokenize and pad to minimum length\n",
    "            tokens = self.tokenizer.tokenize(sentence.lower())\n",
    "            if len(tokens) < self.max_filter_size:\n",
    "                tokens += ['<pad>'] * (self.max_filter_size - len(tokens))\n",
    "            \n",
    "            # numericalize the tokens\n",
    "            tokens_numerical = [self.vocab.stoi[token] if token in self.vocab.stoi else self.vocab.stoi['<unk>']\n",
    "                                for token in tokens]\n",
    "            tokenized_sentences.append(tokens_numerical)\n",
    "            \n",
    "        # run the model, applying a sigmoid because the model outputs logits\n",
    "        logits = self.run_model(tokenized_sentences)\n",
    "        pred = np.apply_along_axis(sigmoid, 1, logits)\n",
    "        \n",
    "        # output two classes\n",
    "        positivity = pred[:, 0]\n",
    "        negativity = 1 - positivity\n",
    "        return np.transpose([negativity, positivity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model runner. max_filter_size is a property of the model\n",
    "model_runner = MovieReviewsModelRunner(model_path, word_vector_path, max_filter_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a sentence of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"A delectable and intriguing thriller filled with surprises\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   2. Which are the classes your model has been trained for? Which of these classes do you want an explanation for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (\"negative\", \"positive\")  # sentiments of the movie reviews\n",
    "explained_class_index = labels.index(\"positive\")  # we are interested why our sentence is classified as having a positive sentiment\n",
    "explained_class_index\n",
    "labels.index('positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  3.  Run dianna with the explainer of your choice, 'LIME', and visualize the output. For textual data use the ```explain_text``` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = dianna.explain_text(model_runner, review, model_runner.tokenizer,'LIME', labels=[explained_class_index])[0]\n",
    "explanation\n",
    "fig, _ = visualization.highlight_text(explanation, model_runner.tokenizer.tokenize(review), colormap='bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image example*\n",
    "\n",
    "Here we apply the general workflow with image data from <img width=\"94\" alt=\"ImageNet_autocrop\" src=\"https://user-images.githubusercontent.com/3244249/152542090-fd78fde1-6dec-43b6-a7ae-eea964b8ae28.png\">. The data item of interest is an image of a bee and we use the [ResNet 50](https://arxiv.org/abs/1512.03385) model trained on [Imagenet](https://image-net.org/download.php) to classify 1000 types of objects. We are interested in which pixels are contributing positively (red) and which - negatively (blue) towards the model's decision to classify the image as a 'bee' and we would like to use the [RISE](http://bmvc2018.org/contents/papers/1064.pdf) explainer:\n",
    "\n",
    "*for a full example see the /Explainers/RISE/rise_imagenet tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explainers**\n",
    "DIANNA supports [RISE](http://bmvc2018.org/contents/papers/1064.pdf), [LIME](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf) and [KernalSHAP](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf) explainability methods. This section demonstrates how to run all the supported explainers for the simple binary classification task of distinguishing the hand-written digits \"0\" and \"1\" on test example from the Binary MNIST dataset, a subset of the [MNIST benchmark](http://yann.lecun.com/exdb/mnist/). It also gives the basics for each of the explainers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  Explaining a Pretrained Binary MNIST Classification Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the Binary MNIST data, the pretrained [binary MNIST model](https://zenodo.org/record/5907177) and chose image to be explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data_path = Path('..', 'dianna','data', 'binary-mnist.npz')\n",
    "data = np.load(data_path)\n",
    "# load testing data and the related labels\n",
    "X_test = data['X_test'].astype(np.float32).reshape([-1, 28, 28, 1]) / 256\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load saved onnx model\n",
    "onnx_model_path = Path('..','dianna','models', 'mnist_model_tf.onnx')\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "# get the output node\n",
    "output_node = prepare(onnx_model, gen_tensor_dict=True).outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Print class and image of a single instance in the test data for preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# class name\n",
    "class_name = ['digit 0', 'digit 1']\n",
    "# instance index\n",
    "i_instance = 3\n",
    "# select instance for testing\n",
    "test_sample = X_test[i_instance]\n",
    "# model predictions with added batch axis to test sample\n",
    "predictions = prepare(onnx_model).run(test_sample[None, ...])[f'{output_node}']\n",
    "pred_class = class_name[np.argmax(predictions)]\n",
    "# get the index of predictions\n",
    "top_preds = np.argsort(-predictions)\n",
    "inds = top_preds[0]\n",
    "print(\"The predicted class for this test image is:\", pred_class)\n",
    "plt.imshow(X_test[i_instance][:,:,0], cmap='gray')  # 0 for channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. KernelSHAP\n",
    "\n",
    "SHapley Additive exPlanations, in short, SHAP, is a model-agnostic explainable AI approach which is used to decrypt the black-box models through estimating the [Shapley values](https://en.wikipedia.org/wiki/Shapley_value), which represent the relevancies of each data feature (image pixel, word in text, etc.). [KernelSHAP](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf) is a variant of SHAP. It is a method that uses the LIME framework to compute Shapley Values, and visualizes the relevance attributions for each pixel/super-pixel by displaying them on an image. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The user needs to specify the number model re-evaluations when explaining each prediction (`nsamples`). A binary mask need to be applied to the image indicating whihc image regiona are hidden. It requires the background color for the masked image, which can be specified by `background`.<br>\n",
    "\n",
    "Performing KernelSHAP for each pixel is inefficient. It is always a good practice to segment the input image to super-pixels and perform computations on them. The user has to specify some keyword arguments related to the segmentation: the (approximate) number of labels in the segmented output image (`n_segments`), and width of Gaussian smoothing kernel for pre-processing for each image dimension (`sigma`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use KernelSHAP to explain the network's predictions\n",
    "shap_values = dianna.explain_image(onnx_model_path, test_sample,\n",
    "                                  method=\"KernelSHAP\", labels=[0], nsamples=1000,\n",
    "                                  background=0, n_segments=200, sigma=0,\n",
    "                                  axis_labels=('height','width','channels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visualize the relevance scores of each pixel on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the explanations\n",
    "# get the range for color bar\n",
    "max_val = np.max([np.max(np.abs(shap_values[i][:,:-1])) for i in range(len(shap_values))])\n",
    "# plot the test image and the attributions on the image for class 0\n",
    "print(f'Explaination for `{pred_class}` with KernelSHAP')\n",
    "\n",
    "fig, _ = visualization.plot_image(shap_values[0], heatmap_cmap='bwr', heatmap_range=(-max_val, max_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "The Shapley scores are estimated using KernelSHAP for models used to categorize the binary MNIST. The example here shows that the KernelSHAP method evaluates the importance of each segmentation/super pixel to the classification and the results are reasonably comparable to the human visual preception of the chosen testing hand-written digit image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. RISE\n",
    "\n",
    "RISE is short for Randomized Input Sampling for Explanation of Black-box Models. It estimates the relevance empirically by probing the model with randomly masked versions of the input image to obtain the corresponding outputs.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "RISE masks random portions of the input image and passes the masked image through the model — the portion that decreases the accuracy the most is the most “important” portion.<br>\n",
    "To call the explainer and generate the relevance scores, the user need to specified the number of masks being randomly generated (`n_masks`), the resolution of features in masks (`feature_res`) and for each mask and each feature in the image, the probability of being kept unmasked (`p_keep`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "relevances = dianna.explain_image(onnx_model_path, test_sample, method=\"RISE\",\n",
    "                                labels=[i for i in range(2)],\n",
    "                                n_masks=5000, feature_res=8, p_keep=.1,\n",
    "                                axis_labels=('height','width','channels'))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visualize the relevance scores for the predicted class on top of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Explaination for `{pred_class}` with RISE')\n",
    "fig, _ = visualization.plot_image(relevances, X_test[i_instance][:,:,0], data_cmap='gray', heatmap_cmap='bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3. LIME\n",
    "\n",
    "LIME (Local Interpretable Model-agnostic Explanations) is an explainable-AI method that aims to create an interpretable model that locally represents the classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# need to preprocess, because we divided the input data by 256 for the models and LIME needs RGB values\n",
    "def preprocess_function(image):\n",
    "    return (image / 256).astype(np.float32)\n",
    "\n",
    "# An explanation is returned for each label, but we ask for just one label so the output is a list of length one.\n",
    "explanation_heatmap = dianna.explain_image(onnx_model_path, test_sample * 256, 'LIME',\n",
    "                                           axis_labels=('height','width','channels'),\n",
    "                                           random_state=2,\n",
    "                                           labels=[i for i in range(2)], preprocess_function=preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Explaination for `{pred_class}` with LIME')\n",
    "fig, _ = visualization.plot_image(explanation_heatmap[0], X_test[i_instance][:,:,0], heatmap_cmap='bwr')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7604e8ec5f09e490e10161e37a4725039efd3ab703d81b1b8a1e00d6741866c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
