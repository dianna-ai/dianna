{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"150\" alt=\"Logo_ER10\" src=\"https://user-images.githubusercontent.com/3244249/151994514-b584b984-a148-4ade-80ee-0f88b0aefa45.png\">\n",
    "\n",
    "## Tensorflow to ONNX conversion\n",
    "This notebook shows how to convert your Tensorflow model to ONNX, the generic format supported by DIANNA. <br>\n",
    "The conversion is complete with the tf2onnx Python package, which supports tensorflow 1.X, 2.X, and tf.keras, and tflite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1bc8ab01d86a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnxruntime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "# In addition to these imports, this notebook\n",
    "# depends on tf2onnx. It is used from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download tensorflow model in GraphDef format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-586471f24950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m fname = tf.keras.utils.get_file(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'mobilenet.tgz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m'https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     extract=True)\n\u001b[1;32m      5\u001b[0m \u001b[0mgraph_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mobilenet_v1_1.0_224/frozen_graph.pb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "fname = tf.keras.utils.get_file(\n",
    "    'mobilenet.tgz',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz',\n",
    "    extract=True)\n",
    "graph_file = os.path.join(os.path.dirname(fname), 'mobilenet_v1_1.0_224/frozen_graph.pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create inference function from frozen graph. (Here tensorflow 2 is used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 15:08:11.337241: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-31 15:08:11.337536: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-31 15:08:11.340018: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "graph_input = 'input'\n",
    "graph_output = 'MobilenetV1/Predictions/Softmax'\n",
    "\n",
    "# helper function to load graph in tf2\n",
    "# taken from https://www.tensorflow.org/guide/migrate\n",
    "def wrap_frozen_graph(graph_def, inputs, outputs):\n",
    "    def _imports_graph_def():\n",
    "        tf.compat.v1.import_graph_def(graph_def, name=\"\")\n",
    "        \n",
    "    wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, [])\n",
    "    import_graph = wrapped_import.graph\n",
    "    return wrapped_import.prune(\n",
    "        tf.nest.map_structure(import_graph.as_graph_element, inputs),\n",
    "        tf.nest.map_structure(import_graph.as_graph_element, outputs)\n",
    "    )\n",
    "\n",
    "graph_def = tf.compat.v1.GraphDef()\n",
    "with open(graph_file, 'rb') as f:\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    \n",
    "func = wrap_frozen_graph(graph_def, inputs=graph_input+':0', outputs=graph_output+':0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model on some random input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 15:08:14.693775: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-01-31 15:08:14.705357: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2304005000 Hz\n"
     ]
    }
   ],
   "source": [
    "input_shape = func.inputs[0].shape\n",
    "input_data = tf.random.normal(shape=input_shape, dtype=tf.float32)\n",
    "pred = func(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert model to tflite and SavedModel format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 15:08:34.284342: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-01-31 15:08:34.481577: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\n",
      "2022-01-31 15:08:34.481647: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\n",
      "2022-01-31 15:08:34.527677: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# convert to tflite\n",
    "tflite_file = '../../dianna/dianna/models/mobilenet.tflite'\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\n",
    "    graph_def_file=graph_file,\n",
    "    input_arrays=[graph_input],\n",
    "    input_shapes={graph_input: input_shape},\n",
    "    output_arrays=[graph_output]\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "with open(tflite_file, 'wb') as f:\n",
    "  f.write(converter.convert())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mobilenet_savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mobilenet_savedmodel/assets\n"
     ]
    }
   ],
   "source": [
    "# create a Trackable object that can be saved as SavedModel\n",
    "class Model(tf.Module):\n",
    "    def __init__(self, function):\n",
    "        super().__init__()\n",
    "        self.function = function\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.function(x)\n",
    "    \n",
    "model = Model(func)\n",
    "\n",
    "# save the model\n",
    "savedmodel_dir = 'mobilenet_savedmodel'\n",
    "tf.saved_model.save(model, savedmodel_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert GraphDef/tflite/SavedModel to onnx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yangliu/miniconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "WARNING:tensorflow:From /home/yangliu/miniconda3/lib/python3.8/site-packages/tf2onnx/tf_loader.py:305: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "2022-01-31 15:09:07,142 - WARNING - From /home/yangliu/miniconda3/lib/python3.8/site-packages/tf2onnx/tf_loader.py:305: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/yangliu/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py:856: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-01-31 15:09:07,142 - WARNING - From /home/yangliu/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py:856: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-01-31 15:09:07,497 - INFO - Using tensorflow=2.4.1, onnx=1.9.0, tf2onnx=1.9.3/1190aa\n",
      "2022-01-31 15:09:07,497 - INFO - Using opset <onnx, 9>\n",
      "2022-01-31 15:09:08,222 - INFO - Computed 0 values for constant folding\n",
      "2022-01-31 15:09:08,796 - INFO - Optimizing ONNX model\n",
      "2022-01-31 15:09:09,597 - INFO - After optimization: Add -14 (27->13), Cast -1 (1->0), Const -13 (83->70), Identity -2 (2->0), Reshape -13 (14->1), Transpose -70 (71->1)\n",
      "2022-01-31 15:09:09,669 - INFO - \n",
      "2022-01-31 15:09:09,669 - INFO - Successfully converted TensorFlow model /home/yangliu/.keras/datasets/mobilenet_v1_1.0_224/frozen_graph.pb to ONNX\n",
      "2022-01-31 15:09:09,669 - INFO - Model inputs: ['input:0']\n",
      "2022-01-31 15:09:09,670 - INFO - Model outputs: ['MobilenetV1/Predictions/Softmax:0']\n",
      "2022-01-31 15:09:09,670 - INFO - ONNX model is saved at ../models/mobilenet_graph.onnx\n",
      "/home/yangliu/miniconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-01-31 15:09:12,917 - INFO - Using tensorflow=2.4.1, onnx=1.9.0, tf2onnx=1.9.3/1190aa\n",
      "2022-01-31 15:09:12,917 - INFO - Using opset <onnx, 9>\n",
      "2022-01-31 15:09:13,290 - INFO - Optimizing ONNX model\n",
      "2022-01-31 15:09:14,140 - INFO - After optimization: Cast -1 (1->0), Const -13 (70->57), Identity -1 (1->0), Reshape -13 (14->1), Transpose -113 (114->1)\n",
      "2022-01-31 15:09:14,196 - INFO - \n",
      "2022-01-31 15:09:14,197 - INFO - Successfully converted TensorFlow model ../models/mobilenet.tflite to ONNX\n",
      "2022-01-31 15:09:14,197 - INFO - Model inputs: ['input']\n",
      "2022-01-31 15:09:14,197 - INFO - Model outputs: ['MobilenetV1/Predictions/Softmax']\n",
      "2022-01-31 15:09:14,197 - INFO - ONNX model is saved at ../models/mobilenet_tflite.onnx\n",
      "/home/yangliu/miniconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-01-31 15:09:17,976 - INFO - Signatures found in model: [serving_default].\n",
      "2022-01-31 15:09:17,977 - INFO - Output names: ['output_0']\n",
      "WARNING:tensorflow:From /home/yangliu/miniconda3/lib/python3.8/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-01-31 15:09:18,734 - WARNING - From /home/yangliu/miniconda3/lib/python3.8/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-01-31 15:09:18,961 - INFO - Using tensorflow=2.4.1, onnx=1.9.0, tf2onnx=1.9.3/1190aa\n",
      "2022-01-31 15:09:18,961 - INFO - Using opset <onnx, 9>\n",
      "2022-01-31 15:09:19,633 - INFO - Computed 0 values for constant folding\n",
      "2022-01-31 15:09:20,189 - INFO - Optimizing ONNX model\n",
      "2022-01-31 15:09:20,935 - INFO - After optimization: Add -14 (27->13), Cast -1 (1->0), Const -13 (83->70), Identity -5 (5->0), Reshape -13 (14->1), Transpose -70 (71->1)\n",
      "2022-01-31 15:09:20,989 - INFO - \n",
      "2022-01-31 15:09:20,989 - INFO - Successfully converted TensorFlow model mobilenet_savedmodel to ONNX\n",
      "2022-01-31 15:09:20,989 - INFO - Model inputs: ['input']\n",
      "2022-01-31 15:09:20,989 - INFO - Model outputs: ['output_0']\n",
      "2022-01-31 15:09:20,989 - INFO - ONNX model is saved at ../models/mobilenet_savedmodel.onnx\n"
     ]
    }
   ],
   "source": [
    "# graphdef to onnx\n",
    "onnx_graphdef = 'mobilenet_graph.onnx'\n",
    "!python -m tf2onnx.convert --graphdef {graph_file} --output {onnx_graphdef} --inputs {graph_input}:0 --outputs {graph_output}:0\n",
    "\n",
    "# tflite to onnx\n",
    "onnx_tflite = 'mobilenet_tflite.onnx'\n",
    "!python -m tf2onnx.convert --tflite {tflite_file} --output {onnx_tflite}\n",
    "\n",
    "# SavedModel to onnx\n",
    "onnx_savedmodel = 'mobilenet_savedmodel.onnx'\n",
    "!python -m tf2onnx.convert --saved-model {savedmodel_dir} --output {onnx_savedmodel} --signature_def serving_default --tag serve\n",
    "\n",
    "# For completeness, this is how to convert a tf.keras model to ONNX:\n",
    "# !python -m tf2onnx.convert --keras {model_dir} --output {output_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate ONNX models and compare to tensorflow output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphdef: True\n",
      "tflite: True\n",
      "SavedModel: True\n"
     ]
    }
   ],
   "source": [
    "models = {'graphdef': onnx_graphdef, 'tflite': onnx_tflite, 'SavedModel': onnx_savedmodel}\n",
    "\n",
    "for model, fname in models.items():\n",
    "\n",
    "    # verify the ONNX model is valid\n",
    "    onnx_model = onnx.load(fname)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "\n",
    "    \n",
    "    # get ONNX predictions\n",
    "    sess = ort.InferenceSession(fname)\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    output_name = sess.get_outputs()[0].name\n",
    "    \n",
    "    onnx_input = {input_name: input_data.numpy()}\n",
    "    pred_onnx = sess.run([output_name], onnx_input)[0]\n",
    "    \n",
    "    print(f\"{model}: {np.allclose(pred_onnx, pred, atol=1e-5)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7604e8ec5f09e490e10161e37a4725039efd3ab703d81b1b8a1e00d6741866c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
